{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "asteroid_prediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PzBhcp5PPsl"
      },
      "source": [
        "# Predicting Diameter and Physical Harm of Asteroids using Machine Learning\n",
        "**Authors** :\n",
        "Colin Campbell (c_c953), Jake Worden (jrw294), Leah Lewis (lrl68) and Ryan Wakabayashi (rjw102)\n",
        "\n",
        "**Abstract** :  [  ]"
      ],
      "id": "3PzBhcp5PPsl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zt_QrpfTPWH_"
      },
      "source": [
        "## Introduction\n",
        "\n",
        " \n",
        "\n",
        " "
      ],
      "id": "zt_QrpfTPWH_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68c58i10PdC_"
      },
      "source": [
        "## Problem Statement \n",
        "Question: How to use machine learning to predict the diameter of asteroids and classify them as physically hazardous.\n",
        "* Asteroid diameter prediction based upon Asteroid_Updated.csv from Kaggle.\n",
        "* Predict whether an asteroid is physically hazardous to Earth. \n",
        "\n",
        "* Success measures:\n",
        "\t* 5 - 10 fold CV accuracy for all models\n",
        "\t* Regression models: R^2 score\n",
        "\t* Classification models: Precision, Recall, ROC/AUC\n",
        "\t\n",
        "* Hope to achieve >85% R^2 for regression models (based upon kaggle responses) and then >=80% precision and recall for the classification models (low goal based on amount of data for imbalanced classes)."
      ],
      "id": "68c58i10PdC_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXUWhVcvPst7"
      },
      "source": [
        "### Related Work\n",
        "\n",
        "**Link to other work:** [Asteroid Diameter Estimators with added difficulty](https://www.kaggle.com/liamkesatoran/asteroid-diameter-estimators-with-added-difficulty)"
      ],
      "id": "HXUWhVcvPst7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1bMdj7LP18_"
      },
      "source": [
        "## Data Management \n",
        "- Describe how did you evaluate your solution\n",
        "- What evaluation metrics did you use?\n",
        "- Describe a baseline system\n",
        "- How much did your system outperform the baseline?\n",
        "- Were there other systems evaluated on the same dataset? How did your system do in comparison to theirs?\n",
        "- Show graphs/tables with results\n",
        "- Error analysis\n",
        "- Suggestions for future improvements\n",
        "\n",
        "Description of the dataset (dimensions, names of variables with their description)"
      ],
      "id": "-1bMdj7LP18_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gx3izY9jQFjT"
      },
      "source": [
        "### Data Gathering\n"
      ],
      "id": "Gx3izY9jQFjT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTrxHhyaQL9I"
      },
      "source": [
        "#### *Motivation*\n",
        "This database was acquired from the Jet Propulsion Laboratory at California Institute of Technology's \"Solar System Dynamics\" on behalf of NASA. This information is related to the orbits, physical and characteristics, and discovery cirumstances for most known natural bodies in our solar system\n"
      ],
      "id": "tTrxHhyaQL9I"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3p_HO92KQR5R"
      },
      "source": [
        "#### *Composition*\n",
        "\t\n",
        "| Feature | Description | Dtype | Null |\n",
        "| ------- | ----------------- | ------ | :------: |\n",
        "| a | Semi-major axis(au) | float64 | 2 |\n",
        "| e | Eccentricity | float64 | 0 |\n",
        "| i | Inclination with respect to x-y ecliptic plain(deg) | float64 | 0 |\n",
        "| om | Longitude of the ascending node | float64 | 0 |\n",
        "| w | Argument of perihelion | float64 | 0 |\n",
        "| q | Perihelion distance(au) | float64 | 0 |\n",
        "| ad | Aphelion distance(au) | float64 | 6 |\n",
        "| per_y | Oribital period(YEARS) | float64 | 1 |\n",
        "| data_arc | Data arc-span(d) | float64 | 15474 |\n",
        "| condition_code | Orbit condition code | object | 867 |\n",
        "| n_obs_used | Number of Observation used | int64 | 0 |\n",
        "| H | Absolute magnitude parameter | float64 | 2689 |\n",
        "| neo | Near Earth Object | object | 6 |\n",
        "| pha | Physically Hazardous Asteroid | object | 16442 |\n",
        "| diameter | Diameter of asteroid(Km) | object | 702078 |\n",
        "| extent | Object bi/tri axial ellipsoid dimensions(Km) | object | 839696 |\n",
        "| albedo | Geometric albedo | float64 | 703305 |\n",
        "| rot_per | Rotation Period(h) | float64 | 820918 |\n",
        "| GM | Standard gravitational parameter, Product of mass and gravitational constant | float64 | 839700 |\n",
        "| BV | Color index B-V magnitude difference | float64 | 838693 |\n",
        "| UB | Color index U-B magnitude difference | float64 | 838735 |\n",
        "| IR | Color index I-R magnitude difference | float64 | 839713 |\n",
        "| spec_B | Spectral taxonomic type(SMASSII) | object | 838048 |\n",
        "| spec_T | Spectral taxonomic type(Tholen) | object | 838734 |\n",
        "| G | Magnitude slope parameter | float64 | 839595 |\n",
        "| moid | Earth minimum orbit intersection distance(au) | float64 | 16442 |\n",
        "| class | Asteroid orbit class | object | 0 |\n",
        "| n | Mean motion(deg/d) | float64 | 2 |\n",
        "| per | Orbital period(d) | float64 | 6 |\n",
        "| ma | Mean anomaly(deg) | float64 | 8 |\n",
        "\n",
        "* Shape: (839714 , 31)\n",
        "* Memory usage: 198.6+ MB\n",
        "\n",
        "**Dataset found here:** [Asteroid_Updated.csv](https://www.kaggle.com/basu369victor/prediction-of-asteroid-diameter?select=Asteroid_Updated.csv)"
      ],
      "id": "3p_HO92KQR5R"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUAle7zJQY7_"
      },
      "source": [
        "### Data Pre-processing, Cleaning, Labeling, and Maintenance \n",
        "\n",
        "- Read in the .csv and visualized .head() and .info()\n",
        "- Checked the number of Null values. If the sum of null values are > 700,000, we dropped the column\n",
        "- If the remaining column has only Nulls, it is dropped\n",
        "- If the remaining rows contain any Nulls, it is dropped"
      ],
      "id": "SUAle7zJQY7_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sb9SJ_WLwNpJ",
        "outputId": "6306659f-4ef6-4605-eaba-2bbfb42c0282"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "sb9SJ_WLwNpJ",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "684738b6",
        "outputId": "cfbb6fc1-8f8d-462e-d3b3-c2c62fc19ad0"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/ML/Project/Asteroid_Updated.csv\")\n",
        "#df= pd.read_csv(\"./Asteroid_Updated.csv\")\n",
        "df.info()"
      ],
      "id": "684738b6",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (0,10,15,16,23,24) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 839714 entries, 0 to 839713\n",
            "Data columns (total 31 columns):\n",
            " #   Column          Non-Null Count   Dtype  \n",
            "---  ------          --------------   -----  \n",
            " 0   name            21967 non-null   object \n",
            " 1   a               839712 non-null  float64\n",
            " 2   e               839714 non-null  float64\n",
            " 3   i               839714 non-null  float64\n",
            " 4   om              839714 non-null  float64\n",
            " 5   w               839714 non-null  float64\n",
            " 6   q               839714 non-null  float64\n",
            " 7   ad              839708 non-null  float64\n",
            " 8   per_y           839713 non-null  float64\n",
            " 9   data_arc        824240 non-null  float64\n",
            " 10  condition_code  838847 non-null  object \n",
            " 11  n_obs_used      839714 non-null  int64  \n",
            " 12  H               837025 non-null  float64\n",
            " 13  neo             839708 non-null  object \n",
            " 14  pha             823272 non-null  object \n",
            " 15  diameter        137636 non-null  object \n",
            " 16  extent          18 non-null      object \n",
            " 17  albedo          136409 non-null  float64\n",
            " 18  rot_per         18796 non-null   float64\n",
            " 19  GM              14 non-null      float64\n",
            " 20  BV              1021 non-null    float64\n",
            " 21  UB              979 non-null     float64\n",
            " 22  IR              1 non-null       float64\n",
            " 23  spec_B          1666 non-null    object \n",
            " 24  spec_T          980 non-null     object \n",
            " 25  G               119 non-null     float64\n",
            " 26  moid            823272 non-null  float64\n",
            " 27  class           839714 non-null  object \n",
            " 28  n               839712 non-null  float64\n",
            " 29  per             839708 non-null  float64\n",
            " 30  ma              839706 non-null  float64\n",
            "dtypes: float64(21), int64(1), object(9)\n",
            "memory usage: 198.6+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b303df2"
      },
      "source": [
        "Print the sum of null values to determine which columns had a high percentage of null values."
      ],
      "id": "2b303df2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adbc5771",
        "outputId": "893de95d-2f38-46e4-b423-08130ab74872"
      },
      "source": [
        "print(df.shape)\n",
        "print(df.isnull().sum())"
      ],
      "id": "adbc5771",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(839714, 31)\n",
            "name              817747\n",
            "a                      2\n",
            "e                      0\n",
            "i                      0\n",
            "om                     0\n",
            "w                      0\n",
            "q                      0\n",
            "ad                     6\n",
            "per_y                  1\n",
            "data_arc           15474\n",
            "condition_code       867\n",
            "n_obs_used             0\n",
            "H                   2689\n",
            "neo                    6\n",
            "pha                16442\n",
            "diameter          702078\n",
            "extent            839696\n",
            "albedo            703305\n",
            "rot_per           820918\n",
            "GM                839700\n",
            "BV                838693\n",
            "UB                838735\n",
            "IR                839713\n",
            "spec_B            838048\n",
            "spec_T            838734\n",
            "G                 839595\n",
            "moid               16442\n",
            "class                  0\n",
            "n                      2\n",
            "per                    6\n",
            "ma                     8\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUGennLHwnwh"
      },
      "source": [
        "When looking at the dataset we noticed that some of the columns with large amounts of null values were actually just categorical columns so we utilized maps and Label Encoding to transform the categorical data to numeric."
      ],
      "id": "aUGennLHwnwh"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eb43880"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder as le\n",
        "\n",
        "df['pha'] = df['pha'].map({'Y': 1, 'N': 0})\n",
        "df['neo'] = df['neo'].map({'Y': 1, 'N': 0})\n",
        "df['condition_code'] = df['condition_code'].map({0: 1, 1: 2, 2: 3, 3: 4, 4: 5, 5: 6, 6: 7, 7: 8, 8: 9, 9: 10, 'D': 11, 'E': 12})\n",
        "df['class'] = le().fit_transform(df['class'])\n",
        "\n",
        "columns = ['name', 'extent', 'albedo', 'rot_per', 'GM', 'BV', 'G', 'UB', 'IR', 'pha', 'spec_B', 'spec_T']\n",
        "df_r = df.drop(columns=columns)"
      ],
      "id": "0eb43880",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2c9d9d6"
      },
      "source": [
        "Drop the columns with high amount of null values. Keeping diameter since it is the target for regression."
      ],
      "id": "e2c9d9d6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98115f66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46282299-ebc6-4a71-b60c-8dceda997638"
      },
      "source": [
        "print(df_r.isnull().sum())"
      ],
      "id": "98115f66",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a                      2\n",
            "e                      0\n",
            "i                      0\n",
            "om                     0\n",
            "w                      0\n",
            "q                      0\n",
            "ad                     6\n",
            "per_y                  1\n",
            "data_arc           15474\n",
            "condition_code    249756\n",
            "n_obs_used             0\n",
            "H                   2689\n",
            "neo                    6\n",
            "diameter          702078\n",
            "moid               16442\n",
            "class                  0\n",
            "n                      2\n",
            "per                    6\n",
            "ma                     8\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b29bedee"
      },
      "source": [
        "After running into issues with incorrect datatypes, we found we needed to go through the data and turn the values into numerical values and those that did not become numeric, were dropped.\n",
        "\n",
        "We then printed the sum of nulls to check that the dataframe had no null values."
      ],
      "id": "b29bedee"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ec9d609",
        "outputId": "b6e393ae-db72-44ae-f530-ea7640a38955"
      },
      "source": [
        "df_r = df_r.apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
        "df_r['diameter'].astype(float)\n",
        "df_r.dropna(how='all', axis=1, inplace=True)\n",
        "df_r.dropna(how='any', axis=0, inplace=True)\n",
        "\n",
        "print(df_r.shape)\n",
        "print(df_r.isnull().sum())\n",
        "print(df_r.dtypes)"
      ],
      "id": "8ec9d609",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(127910, 19)\n",
            "a                 0\n",
            "e                 0\n",
            "i                 0\n",
            "om                0\n",
            "w                 0\n",
            "q                 0\n",
            "ad                0\n",
            "per_y             0\n",
            "data_arc          0\n",
            "condition_code    0\n",
            "n_obs_used        0\n",
            "H                 0\n",
            "neo               0\n",
            "diameter          0\n",
            "moid              0\n",
            "class             0\n",
            "n                 0\n",
            "per               0\n",
            "ma                0\n",
            "dtype: int64\n",
            "a                 float64\n",
            "e                 float64\n",
            "i                 float64\n",
            "om                float64\n",
            "w                 float64\n",
            "q                 float64\n",
            "ad                float64\n",
            "per_y             float64\n",
            "data_arc          float64\n",
            "condition_code    float64\n",
            "n_obs_used          int64\n",
            "H                 float64\n",
            "neo               float64\n",
            "diameter          float64\n",
            "moid              float64\n",
            "class               int64\n",
            "n                 float64\n",
            "per               float64\n",
            "ma                float64\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X14AJxohzwq3"
      },
      "source": [
        "columns_c = ['name', 'extent', 'albedo', 'rot_per', 'GM', 'BV', 'G', 'UB', 'IR', 'spec_B', 'spec_T', 'diameter']\n",
        "df_c = df.drop(columns=columns_c)\n",
        "print(df_c.shape)\n",
        "print(df_c.isnull().sum())\n"
      ],
      "id": "X14AJxohzwq3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtIMDbA4z1O3"
      },
      "source": [
        "#df_c = df_c.apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
        "df_c.dropna(how='all', axis=1, inplace=True)\n",
        "df_c.dropna(how='any', axis=0, inplace=True)\n",
        "\n",
        "print(\"Classification set: \", df_c.shape)\n",
        "print(df_c.isnull().sum())\n",
        "print(df_c.dtypes)"
      ],
      "id": "FtIMDbA4z1O3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6crj_N_RgyL"
      },
      "source": [
        "### Exploratory Data Analysis \n",
        "\n",
        "Methods\n",
        "* Visualization\n",
        "  * seaborn pairplot - Used to see the interactions between features and the target values for both regression and classification\n",
        "  * correlation heatmap - Used as a secondary source to visualize the strength of correlation between features and targets. \n",
        "* ANOVA\n",
        "  * We utilized ANOVA to select and transform our dataset to keep the top 10 features in the dataset.\n",
        "\n",
        "All of our methods came up with similar conclusions as to what features were deemed most important to our targets. Because of this, we saw an increase in accuracy values once we transformed the feature set to only contain these top 10."
      ],
      "id": "D6crj_N_RgyL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbPZzVHZVb9H"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.set(style=\"dark\", color_codes=True)\n",
        "g = sns.pairplot(data=df_r)\n",
        "\n",
        "plt.show()"
      ],
      "id": "nbPZzVHZVb9H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrNI-UE6Vc9F"
      },
      "source": [
        "plt.subplots(figsize = (16,15))\n",
        "sns.heatmap(df_r.corr(),annot=True, annot_kws={'size':10})\n",
        "# per, moid, H, per_y, ad, q, a"
      ],
      "id": "BrNI-UE6Vc9F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tV0YapiU3ELp"
      },
      "source": [
        "sns.set(style=\"dark\", color_codes=True)\n",
        "g = sns.pairplot(data=df_c)\n",
        "\n",
        "plt.show()"
      ],
      "id": "tV0YapiU3ELp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6Q_CDL53JlH"
      },
      "source": [
        "plt.subplots(figsize = (16,15))\n",
        "sns.heatmap(df_c.corr(),annot=True, annot_kws={'size':10})"
      ],
      "id": "j6Q_CDL53JlH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c94d9c5"
      },
      "source": [
        "### Determine Feature Selection"
      ],
      "id": "7c94d9c5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rw-kPDcizg5w"
      },
      "source": [
        "####Regression"
      ],
      "id": "rw-kPDcizg5w"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "027cbd55"
      },
      "source": [
        "y = df_r.diameter\n",
        "x = df_r.drop(columns=['diameter'])"
      ],
      "id": "027cbd55",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68ddb3dd"
      },
      "source": [
        "from sklearn.feature_selection import SelectKBest\n",
        "# ANOVA on features on target to determine which features are significant\n",
        "anova = SelectKBest(k=10)\n",
        "# fitting ANOVA model with features and target\n",
        "transX = anova.fit_transform(x, y)\n",
        "\n",
        "for i in range(len(x.columns)):\n",
        "    print(f'{x.columns[i]}: {anova.scores_[i]}')\n",
        "\n",
        "print(transX.shape)"
      ],
      "id": "68ddb3dd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-BRrGPzuEss"
      },
      "source": [
        "Based on the ANOVA, heatmap, and pairplot we can see that there are multiple features that have high importance when determining diameter. \n",
        "\n",
        "\n",
        "1.   per / per_y\n",
        "2.   ad\n",
        "3.   a\n",
        "4.   H\n",
        "5.   q\n",
        "6.   moid\n",
        "7.   neo\n",
        "8.   n\n",
        "9.   class\n",
        "10.  data_arc"
      ],
      "id": "W-BRrGPzuEss"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oyfawPRzkPx"
      },
      "source": [
        "####Classification"
      ],
      "id": "1oyfawPRzkPx"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMGWD-cHz40t"
      },
      "source": [
        "x_c = df_c.drop(columns=['pha'])\n",
        "y_c = df['pha']"
      ],
      "id": "yMGWD-cHz40t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yv1zEcxdzmG6"
      },
      "source": [
        "x_c = df_c.drop(columns='pha')\n",
        "print(\"Classification ANOVA \", x_c.shape)\n",
        "y_c = df_c['pha']\n",
        "print(y_c.shape)\n",
        "\n",
        "# ANOVA on features on target to determine which features are significant\n",
        "anova = SelectKBest(k=10)\n",
        "# fitting ANOVA model with features and target\n",
        "anova.fit(x_c, y_c)\n",
        "\n",
        "for i in range(len(x_c.columns)):\n",
        "   print(f'{x_c.columns[i]}: {anova.scores_[i]}')\n"
      ],
      "id": "Yv1zEcxdzmG6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIGpx4HPRnRc"
      },
      "source": [
        "## Machine Learning Approaches\n",
        "\n",
        "We tried multiple models for our regression prediction. When it came to parameter tuning, some took an excessive amount of resources and we chose to look elsewhere. If a model performed badly after gridsearch and 10-fold cross validation, we looked into more data and other methods of improving but inevitably found other models that performed well with less tuning and less computational cost. \n",
        "\n",
        "**All attempted models**\n",
        "\n",
        "Regression:\n",
        "*   Random Forest \n",
        "*   KNN\n",
        "*   SGD\n",
        "*   Lasso\n",
        "*   Ridge\n",
        "*   SVR\n",
        "\n",
        "Classification\n",
        "*   Logistic Regression\n",
        "\n"
      ],
      "id": "UIGpx4HPRnRc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qghs-LGORqlX"
      },
      "source": [
        "### Describe the ML methods that you used and the reasons for their choice. \n",
        "What is the family of machine learnign algorithms you are using and why? \n",
        "* Supervised or Unsupervised?\n",
        "* Regression or classification?"
      ],
      "id": "Qghs-LGORqlX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDrmstW-Rtcz"
      },
      "source": [
        "### Justify ML algorithms in terms of the problem itself and the methods you want to use. \n",
        "* How did you employ them? \n",
        "* What features worked well and what didn't?\n",
        "* Provide documentation for integration  "
      ],
      "id": "dDrmstW-Rtcz"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRkCPS8YRvg4"
      },
      "source": [
        "### Tools and Infrastructure Tried and Not Used\n",
        "\n",
        "Describe any tools and infrastruicture that you tried and ended up not using.\n",
        "What was the problem? \n",
        "Describe infrastructure used. "
      ],
      "id": "wRkCPS8YRvg4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yg0_ctU3nYie"
      },
      "source": [
        "### **Regression Models for predicting diameter**"
      ],
      "id": "Yg0_ctU3nYie"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqgFkEp-tbU2"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(transX ,y, test_size = 0.2, random_state=1)"
      ],
      "id": "SqgFkEp-tbU2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7924b2f2"
      },
      "source": [
        "#### KNN Regressor"
      ],
      "id": "7924b2f2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf134a76"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "knn = KNeighborsRegressor()\n",
        "knn.fit(x_train, y_train)\n",
        "print(\"Base KNN score:\", knn.score(x_test,y_test))"
      ],
      "id": "cf134a76",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6b495142"
      },
      "source": [
        "\"\"\"\n",
        "param_grid = {'n_neighbors' : [3],\n",
        "                'weights' : ['distance'],\n",
        "                'metric' : ['chebyshev','euclidean', ]\n",
        "                 }\n",
        "\n",
        "gs = GridSearchCV(estimator=knn, param_grid=param_grid, scoring='r2', cv=10)\n",
        "gs = gs.fit(x_train, y_train)\n",
        "print(gs.best_params_)\n",
        "print(gs.best_score_)\n",
        "\"\"\""
      ],
      "id": "6b495142",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7IGQDNiotwC"
      },
      "source": [
        "After running the grid search with the parameter grid above, the following parameters were selected as the best performing\n",
        "\n",
        "*   n_neighbors: 3\n",
        "*   weights: 'distance'\n",
        "*   metric: 'euclidean'"
      ],
      "id": "J7IGQDNiotwC"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6773a82"
      },
      "source": [
        "\n",
        "bestKNN = KNeighborsRegressor(n_neighbors=3, weights='distance', metric='euclidean')\n",
        "bestKNN.fit(x_train, y_train)\n",
        "bestKNN.score(x_test,y_test)"
      ],
      "id": "c6773a82",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a9a5c68"
      },
      "source": [
        "bestKNN = KNeighborsRegressor(n_neighbors=4, weights='distance', metric='euclidean')\n",
        "bestKNN.fit(x_train, y_train)\n",
        "bestKNN.score(x_test,y_test)"
      ],
      "id": "0a9a5c68",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "027b137a"
      },
      "source": [
        "bestKNN = KNeighborsRegressor(n_neighbors=5, weights='distance', metric='euclidean')\n",
        "bestKNN.fit(x_train, y_train)\n",
        "bestKNN.score(x_test,y_test)"
      ],
      "id": "027b137a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31f5daf5"
      },
      "source": [
        "#### Lasso Regressor\n"
      ],
      "id": "31f5daf5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3a58d1e"
      },
      "source": [
        "from sklearn.linear_model import  Lasso\n",
        "\n",
        "lasso_base = Lasso()\n",
        "lasso_base.fit(x_train, y_train)\n",
        "pred_lasso = lasso_base.predict(x_test)\n",
        "print(\"Base lasso score: \", lasso_base.score(x_test,y_test))"
      ],
      "id": "a3a58d1e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2KbZevbtA-A"
      },
      "source": [
        "\"\"\"\n",
        "param_grid = [{'alpha':[1, 10, 25, 100, 500]},\n",
        "              {'max_iter':[1000, 100, 500, 5000, 25000, 100000]},\n",
        "              {'tol':[1e-4, 1e-6, 1e-10]},\n",
        "              {'selection':['cyclic','random']},\n",
        "              {'random_state':[10, 75, 200, 500, 1337, 5000]}]\n",
        "gs = GridSearchCV(estimator=lasso, param_grid=param_grid, scoring='r2', cv=7, return_train_score=True)\n",
        "lasso.fit(x_train, y_train)\n",
        "gs = gs.fit(x_train, y_train)\n",
        "print(gs.best_params_)\n",
        "print(gs.best_score_)\n",
        "\"\"\""
      ],
      "id": "W2KbZevbtA-A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4Fmep5zxQJG"
      },
      "source": [
        ""
      ],
      "id": "i4Fmep5zxQJG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b63ce81"
      },
      "source": [
        "Parameter Tuning Results with Lasso\n",
        "\n",
        "Alpha:\n",
        "    When an alpha of 0 was available, it was always chosen and values were 0.36 for alpha = 0 and -.37 for alpha = 0.5. Any larger alpha values were not selected for by param_grid.\n",
        "\n",
        "Selection:\n",
        "    When alpha was not less than one, selection was chosen for, and 'random' was selected for as the type of selection. This gave a score of -.49.\n",
        "\n",
        "Max_iter:\n",
        "    When not tuning for selection, a max_iter of 1000 was selected with a score of -.49.\n",
        "\n",
        "tol (tolerance):\n",
        "    When not tuning for selection or max_iter, a tol of 0.0001 was chosen for with a score of -.49 once again.\n",
        "\n",
        "Random_state:\n",
        "    When not tuning for selection,max_iter, or tol a random state of 10 was chosen with the same score of -.49.\n",
        "\n",
        "cv values:\n",
        "    Any cv value that was under 7 gave a result that was larger than 1, getting larger the closer cv got to 0. The above values were calculated with cv=8, but with cv=7 a value of -.56 was obtained. The higher cv value the closer to 0 we got"
      ],
      "id": "4b63ce81"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5beb5a30"
      },
      "source": [
        "#### Gradient Boosting Regressor"
      ],
      "id": "5beb5a30"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5beee6aa"
      },
      "source": [
        "A fitted GBR model using it's default paramters. n_estimators = 100"
      ],
      "id": "5beee6aa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50b9a77a"
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "gbr = GradientBoostingRegressor(random_state=0)\n",
        "gbr.fit(x_train, y_train)\n",
        "print(\"Base GBR score: \",gbr.score(x_test, y_test))"
      ],
      "id": "50b9a77a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7ffce05"
      },
      "source": [
        "\"\"\"\n",
        "param_grid = {'n_estimators' : [105],\n",
        "             }\n",
        "gbr = GradientBoostingRegressor()\n",
        "gs = GridSearchCV(estimator=gbr, param_grid=param_grid, scoring='r2', cv=10)\n",
        "gs = gs.fit(x_train, y_train)\n",
        "print(gs.best_params_)\n",
        "print(gs.best_score_)\n",
        "\"\"\""
      ],
      "id": "b7ffce05",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkgtD2jEom6u"
      },
      "source": [
        "After running the grid search with the parameter grid above, the following parameters were selected as the best performing\n",
        "\n",
        "*   n_estimators: 105"
      ],
      "id": "xkgtD2jEom6u"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53e6fb90"
      },
      "source": [
        "gbr = GradientBoostingRegressor(n_estimators=105, random_state=0)\n",
        "gbr.fit(x_train, y_train)\n",
        "print(\"Optimal GBR score: \",gbr.score(x_test, y_test))"
      ],
      "id": "53e6fb90",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5ZT30RhmGry"
      },
      "source": [
        "####Random Forest Regressor"
      ],
      "id": "A5ZT30RhmGry"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3QZda4-mLvW"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "rf_base = RandomForestRegressor()\n",
        "\n",
        "rf_base.fit(x_train, y_train)\n",
        "pred_rf_base = rf_base.predict(x_test)\n",
        "print(\"Base RF score: \",rf_base.score(x_test,y_test))"
      ],
      "id": "L3QZda4-mLvW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KzjpCCamVOI"
      },
      "source": [
        "\"\"\"\n",
        "param_grid = [{'n_estimators' : [100, 150, 200, 250, 300],\n",
        "               'max_depth' : [None, 10, 20, 30, 40],\n",
        "               'min_samples_split' : [2, 3, 4]}]\n",
        "\n",
        "gs = GridSearchCV(estimator=rf, param_grid=param_grid, scoring='r2', cv=10, return_train_score=True)\n",
        "gs.fit(xtrain,ytrain)\n",
        "print(\"\\nBest parameters: \",gs.best_params_)\n",
        "print(gs.best_score_)\n",
        "\"\"\""
      ],
      "id": "0KzjpCCamVOI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-1gYlnjmgug"
      },
      "source": [
        "After running the grid search with the parameter grid above, the following parameters were selected as the best performing\n",
        "\n",
        "*   n_estimators: \n",
        "*   max_depth\n",
        "*   min_samples_split:\n"
      ],
      "id": "M-1gYlnjmgug"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IePLtoCmdhr"
      },
      "source": [
        "rf_opt = RandomForestRegressor(n_estimators=150)\n",
        "\n",
        "rf_opt.fit(x_train, y_train)\n",
        "pred_rf_opt = rf_opt.predict(x_test)\n",
        "print(\"Optimal RF score: \", rf_opt.score(x_test,y_test))"
      ],
      "id": "6IePLtoCmdhr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vnCK6gOnlw3"
      },
      "source": [
        "###**Classification models for predicting if an asteroid is hazardous:**"
      ],
      "id": "9vnCK6gOnlw3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJ4qE_V_vFrC"
      },
      "source": [
        "#### Logistic Regression"
      ],
      "id": "rJ4qE_V_vFrC"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3nVkTPanwEP"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression as lr\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_c, y_c, test_size=.2, stratify=y_c, random_state=1)\n",
        "\n",
        "lreg = lr()\n",
        "lreg.fit(x_train, y_train)\n",
        "pred = lreg.predict(xtest)\n",
        "print(\"Log Reg: \", lreg.score(pred,y_test))"
      ],
      "id": "O3nVkTPanwEP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoXpypmARyRH"
      },
      "source": [
        "## Experiments\n",
        "\n",
        "Give a detailed summary of the results of your work.\n",
        "\n",
        " * Setup - Here is where you specify the exact performance measures you used.  \n",
        "   * Describe the data used in experiment for presenting dataset: Datasheets for Dataset template \n",
        "   * Describe your accuracy or quality measure, and your performance (runtime or throughput) measure. \n",
        "   \n",
        " * Please use visualizations whenever possible. Include links to interactive visualizations if you built them. \n",
        " \n",
        " * You can also submit a separated notebook as an appendix to your report if that makes the visualization/interaction task easier. \n",
        "   * It would be reasonable to submit your report as a notebook, but please make sure it runs on one of the two standard environments, and that you include any required files. "
      ],
      "id": "xoXpypmARyRH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwCLvvZQR1o9"
      },
      "source": [
        "## Conclusion\n",
        "In this section give a high-level summary of your results. If the reader only reads one section of the report, this one should be it, and it should be self-contained.  You can refer back to the Experiments Section for elaborations. This section should be less than a page. In particular emphasize any results that were surprising."
      ],
      "id": "EwCLvvZQR1o9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dj7T851TR3I_"
      },
      "source": [
        "## References\n",
        "List the references that cited in your project."
      ],
      "id": "Dj7T851TR3I_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGKLyQzqR5ar"
      },
      "source": [
        "## Appendix## \n",
        "\n",
        "Explain the contributions of each member to the project. Include all supporting materials, e.g., additional figures/tables, Python code technical derivations."
      ],
      "id": "HGKLyQzqR5ar"
    }
  ]
}